# -*- coding: utf-8 -*-

import konlpy
from konlpy.tag import Twitter
import nltk 
import pickle
import numpy
from numpy._distributor_init import NUMPY_MKL
import operator
from gensim import corpora, models
import gensim
import math
from collections import Counter
import os, re, copy
import sys
import io
import matplotlib.pyplot as plt
from wordcloud import Wordcloud          
           
           
twitter = Twitter()

file_path = "C:\ysjung\KISA\data\TEST"
file_name = "A128.mlf"

with io.open(file_path + "\\" + file_name, 'r', encoding='utf8') as file:
   data = file.read()

print(type(data))  # str

# 한글만 추출
data_han = re.compile('[^ ㄱ-ㅣ가-힣]+')  # 한글과 띄어쓰기를 제외한 모든 글자(필터 조건)

han_suss = data_han.sub('', data)
han_fail = data_han.findall(data)

print(type(han_suss))  # str

tokens = twitter.pos(han_suss, norm=True, stem=True)   # 정규화의 의미..?
# 동사, 형용사, 명사만 추출
tokens = [word for word, part in tokens if part == "Verb" or part == "Adjective" or part == "Noun"] 
# [word for word, part in tokens if part == "Verb" or part == "Adjective" or part == "Noun"] 

print(tokens)

# 빈도수 계산 , 정렬
frequency = {}
for i in tokens:
    if i not in frequency:
        frequency[i] = 1
    else:
        frequency [i] += 1 

tokens_sort = sorted(frequency.items(), key=lambda x:x[1], reverse=True)

print(tokens_sort)


'''
#tokens = twitter.pos(han_suss)                        # 품사 tagging
#tokens = twitter.nouns(han_suss)                      # 명사만 추출
### 명사(Noun), 형용사(Adjective), 동사(Verb) 추출시
tokens = [word for word, part in tokens if part == "Verb" or part == "Adjective" or part == "Noun"]
#####  

print(tokens)
print()
print()

# 빈도수 계산 , 정렬
frequency = {}
for i in tokens:
    if i not in frequency:
        frequency[i] = 1
    else: frequency[i] += 1

data_sort = sorted(frequency.items(), key=lambda x:x[1], reverse=True)  # 정렬

print(data_sort)

'''

# http://konlpy.org/ko/v0.4.3/examples/wordcloud/

##############################################################
## 20171031_TFIDF
##############################################################
# -*- coding: utf-8 -*-

import konlpy
from konlpy.tag import Twitter
import nltk
import pickle
import numpy
from numpy._distributor_init import NUMPY_MKL
import operator
from gensim import corpora, models
import gensim
import math
from collections import Counter
import os, re, copy
import sys
import io
import matplotlib.pyplot as plt
import pytagcloud


twitter = Twitter()

file_path = "C:\ysjung\KISA\data\TEST"
file_name = "A128.mlf"

with io.open(file_path + "\\" + file_name, 'r', encoding='utf8') as file:
   data = file.read()

#print(type(data))  # str

# 한글만 추출
data_han = re.compile('[^ ㄱ-ㅣ가-힣]+')  # 한글과 띄어쓰기를 제외한 모든 글자(필터 조건)

han_suss = data_han.sub('', data)
han_fail = data_han.findall(data)

#print(type(han_suss))  # str

tokens = twitter.pos(han_suss, norm=True, stem=True)   # 정규화의 의미..?
# 동사, 형용사, 명사만 추출
tokens = [word for word, part in tokens if part == "Noun"]
#tokens = [word for word, part in tokens if part == "Verb" or part == "Adjective" or part == "Noun"]
#[word for word, part in tokens if part == "Verb" or part == "Adjective" or part == "Noun"]

print(tokens)

'''
# 불용어 사전 및 사용자 전처리 추가 한다
def text_preprocessing(sentence):
    sentence = re.sub(r"\u3000","", sentence) 
    sentence = re.sub(r"[0-9]+","", sentence)  
    return sentence 
'''

# TF
# 일반적인 TF 방법
# 로그 스케일 빈도
# 증가 빈도

# IDF
#

# TF*IDF 


###################################################
import math
# from text.blob import TextBlob as tb

def tf(word, blob):
    return blob.words.count(word) / len(blob.words)

def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob)

def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))

def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)



###################################################
def n_containing(word, whole_document):
    return sum(1 for doc in whole_document if word in doc )

def idf(word, whole_document):
    return math.log(len(whole_document) / (1 + n_containing(word, whole_document)))

def tfidf(doc_tokens,whole_document):
    
    tf_idf = {}

    token_counts = Counter(doc_tokens)

    for token in token_counts:
        tf = token_counts[token] / len(doc_tokens)
        tf_idf[token] = tf * idf(token, whole_document)
    if (len(doc_tokens)>150):
        tf_idf = sorted(tf_idf.items(), key=operator.itemgetter(1), reverse=True)
        if( len(tf_idf) > 70 ):
            for word in tf_idf[70:]:
                while (word[0] in doc_tokens):
                    doc_tokens.remove(word[0])
    # if( len(tf_idf) > 20 ):
    #     for word in tf_idf[:20]:
    #         while (word[0] in doc_tokens):
    #             doc_tokens.remove(word[0])
    # if( len(tf_idf) > 40 ):
    #     for word in tf_idf[40:]:
    #         while (word[0] in doc_tokens):
    #             doc_tokens.remove(word[0])
    return doc_tokens





