############################################################
DGA\run.py
  -> 전체 실행
    1. DGA_classification.bigram 호출 (DGA\DGA_classification\bigram.py)
       DGA_classification.lstm 호출 (DGA\DGA_classification\lstm.py)

    2. bigram.py
       - data_generator 호출(get_data())
       - bigram 모델 생성 및 학습

       lstm.py
       - data_generator 호출(get_data())
       - lstm 모델 생성 및 학습


    3. data_generator.py (DGA\data_generator.py)
       - DGA_generator.* 호출(DGA 알고리즘)

    4. DGA_generator.* (DGA\DGA_generators\*.*)
       - DGA 알고리즘 분석결과 10개

  -> 시각화(gragh) 생성


#####
DGA\DGA_classification\bigram.py
  -> bigram 구현 code

DGA\DGA_classification\lstm.py
  -> lstm 구현 code

DGA\data_generator.py
  -> bigram or lstm 수행 위한 train set / test set 생성

#####
DGA\DGA_generators\*.*
  -> DGA 알고리즘 분석
  -> 현재 10개 구현
  -> 향후 reverse engineering 으로 추가 및 확장 가능


############################################################
딥러닝을 통한 DGA 분류 및 성능비교(가칭)


요약
   알려져있는 DGA 패턴을 분석해(open source) 악성 도메인을 생성 후 딥러닝 기술로 학습하고,
   악성 도메인 분류기를 구현 합니다.
   bigram 과 lstm 알고리즘을 통해 도메인 분류(Black / White) 했으며,
   성능 테스트하고 그 결과를 토의 합니다.


1. 소개
   오늘날 사이버 범죄의 근원인 악성코드는 광범위한 연구 노력에도 불구하고 여전히 심각하며 감염은 계속 증가하고 있습니다.
   또한 해커들이 악성코드에 이식한 DGA는 더욱더 진화하면서 악성 도메인을 생성하고 있으며,
   그것을 극복하기 위한 대안도 지속하는 발전하는 것이 매우 중요합니다.
   따라서 악석코드로부터 감염된 좀비PC 와 제어서버를 효과적으로 차단하는 선제대응 기술을 필요로 합니다.
   본 실험에서는 악성코드 문제를 해결하기 위해 알려져있는 DGA 를 분석해 패턴을 파악하고
   그 패턴에 의해 생성된 악성 도메인을 딥러닝 기술로 학습하고 제어서버 도메인을 분류합니다.
   이는 미래의 악성 도메인 사전 예측을 의미하며 향후 악성 도메인 자동 분류기로 발전할 수 있습니다.
   또한 해당 알고리즘별 성능 및 분류 결과를 토의 합니다.


2. 연구
   - DGA
     - 정의(c&c 관계)
       악성 코드에 감염된 좀비PC 가 제어서버(c&c 서버)에 접속할때 DNS를 요청하게 되는데 해커는 악성코드에 DGA를 적용 합니다.
       (DNS는 제어 서버의 실제 주소와 웹사이트이 별칭(domain)을 연결시켜주는 서비스)
       DGA는 대량의 도메인을 생성하고 이를 제어서버에 접속하기 위한 도메인으로 사용하며,
       이때 제어서버의 주소를 계속 바꾸게 되어 DNS 행위 분석을 통해 악용당하고 있는 도메인에 대한 접근을 차단하거나 끊어버려도
       다른 도메인을 사용하게 되어 제어서버의 차단을 피하게 됩니다.

       예를 들어 제어서버의 도메인을 어제는 cnc1.com 로 사용하고, 오늘은 cnc2.com 를 사용하고,
       내일은 cnc3.com을 사용해(주기적인 변경) 오늘 제어서버 접속이 차단 되더라도 내일은 접속이 가능한 식이기
       때문에 항구적인 접속 차단이 어렵게 되는 것입니다.

   - bigram
     - 정의 (unigram / bigram / n-gram)
       유니그램 모형은 문장내 하나하나의 단어는 어떤 확률 분포에서 독립적으로 추출되었다고 가정 합니다.
       쉽게 말해 한 면마다 단어가 쓰인 거대한 주사위를 던져서, 온 단어들로 문장이 이뤄졌다고 보는 것 입니다.
       단어마다 문장내 출현 빈도를 알고 있으므로, 단어들을 이 확률에 따라 랜덤하게 생성할 수 있습니다.

       유니그램 모형의 확장 모형으로 바이그램(bigram) 모형이 있습니다.
       '바이(bi-)'는 '둘'이라는 뜻 입니다.
       한 단어가 나타날 확률이 앞 단어에 영향을 받는다고 가정하는 것입니다.
       같은 방식으로 바이그램 모형을 확장해서 트라이그램(trigram) 모형을 만들 수도 있습니다.
       '트라이(tri-)'는 3이라는 뜻이므로 이번엔 한 단어가 나타날 확률이 바로 앞단어만이 아니라 그 앞단어에도 영향을 받습니다.

       유니그램, 바이그램, 트라이그램 같은 모형을 모두 합쳐서 N그램이라고 부르는데 N을 늘리면 늘릴 수록
       점점 더 말같은 소릴 하는 모형을 만들 수 있지만 N그램만으로는 완전한 문장을 쓸 수가 없습니다.
       N그램 모형은 확율로만 처리하고 의미론적인 부분을 처리하는 부분이 전혀 없기 때문에 아무리 N이 늘어나도
       구문론적인 관계를 전혀 포착하지 못하기 때문이다.

       예를 들어 "눈이 아파"라는 문장이 있다면 이 '눈'은 펄펄 내리는 눈(雪)이 아니라
       사람의 몸에 있는 눈(目)일 확률이 높다는 것 정도는 N그램 모형으로 식별 할 수 있습니다.

   - LSTM
     - 정의 (RNN 과 LSTM)
       RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류입니다.
       음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델로 알려져 있으며,
       시계열 데이터 형태를 갖는 데이터의 패턴을 인식하는 인공신경망 입니다.
       RNN은 지금 들어온 입력 데이터와 과거에 입력 받았던 데이터를 동시에 고려하며
       마치 머릿속에 기억을 저장하고 있듯이 은닉층에 기억을 저장합니다.
       사람은 생각하고 판단하는 과정에서 과거의 기억에 의존하는데, RNN이 하는 일도 이와 비슷합니다.

       RNN의 변형인 LSTM은 오차의 그라디언트가 시간을 거슬러서 잘 흘러갈 수 있도록 도와줍니다.
       backprop하는 과정에서 오차의 값이 더 잘 유지되는데, 결과적으로 1000단계가 넘게 거슬러 올라갈 수 있습니다.
       이렇게 그라디언트가 잘 흘러간다는 것은 다시 말해 RNNs가 더 오래 전 일도 잘 기억한다는 의미입니다.
       LSTM 유닛은 여러 개의 게이트(gate)가 붙어있는 셀(cell)로 이루어져있으며 이 셀의 정보를
       새로 저장/셀의 정보를 불러오기/셀의 정보를 유지하는 기능이 있습니다(컴퓨터의 메모리 셀과 비슷합니다).
       셀은 셀에 연결된 게이트의 값을 보고 무엇을 저장할지, 언제 정보를 내보낼지, 언제 쓰고 언제 지울지를 결정합니다.
       이 게이트가 열리거나(1) 닫히는(0) 디지탈이 아니라 아날로그라는 점 주의하셔야 합니다.
       즉, 각 게이트는 0에서 1사이의 값을 가지며 게이트의 값에 비례해서 여러 가지 작동을 합니다.
       각 게이트가 갖는 값, 즉 게이트의 계수(또는 가중치, weight)는 은닉층의 값과 같은 원리로 학습됩니다.
       즉 게이트는 언제 신호를 불러올지/내보낼지/유지할지를 학습하며 이 학습과정은 출력의 오차를 이용한
       경사 하강법(gradient descent)을 사용합니다.


3. 실험
   - 실험 모형 -> 그림으로..
   - data의 이해
   - 전처리
   - train set / test set


4. 실험 및 결과
   - 실험
   - 결과


5. 결론




############################################################

